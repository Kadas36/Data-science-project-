{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import get_dummies\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "import sklearn\n",
    "import string\n",
    "import scipy\n",
    "import numpy\n",
    "import nltk\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib: 3.0.2\n",
      "scipy: 1.1.0\n",
      "seaborn: 0.9.0\n",
      "pandas: 0.23.4\n",
      "numpy: 1.15.4\n",
      "Python: 3.7.1 (default, Dec 14 2018, 19:28:38) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission_stage_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_pronoun_df = pd.read_csv('test_stage_1.tsv', delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset                  A  A-offset                B  B-offset  \\\n",
       "0             274     Cheryl Cassidy       191          Pauline       207   \n",
       "1             284          MacKenzie       228    Bernard Leach       251   \n",
       "2             265            Angeloz       173       De la Sota       246   \n",
       "3             321               Hell       174  Henry Rosenthal       336   \n",
       "4             437  Kitty Oppenheimer       219           Rivera       294   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n",
       "1      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n",
       "2  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n",
       "3          http://en.wikipedia.org/wiki/Crime_(band)  \n",
       "4        http://en.wikipedia.org/wiki/Jessica_Rivera  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 9 columns):\n",
      "ID                2000 non-null object\n",
      "Text              2000 non-null object\n",
      "Pronoun           2000 non-null object\n",
      "Pronoun-offset    2000 non-null int64\n",
      "A                 2000 non-null object\n",
      "A-offset          2000 non-null int64\n",
      "B                 2000 non-null object\n",
      "B-offset          2000 non-null int64\n",
      "URL               2000 non-null object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 140.7+ KB\n"
     ]
    }
   ],
   "source": [
    "gendered_pronoun_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Zoe Telford -- played the police officer girlf...\n",
      "1    He grew up in Evanston, Illinois the second ol...\n",
      "2    He had been reelected to Congress, but resigne...\n",
      "3    The current members of Crime have also perform...\n",
      "4    Her Santa Fe Opera debut in 2005 was as Nuria ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(gendered_pronoun_df.Text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set :  (2000, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train set : \",gendered_pronoun_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'B',\n",
       "       'B-offset', 'URL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "    flag=df.isna().sum().any()\n",
    "    if flag==True:\n",
    "        total = df.isnull().sum()\n",
    "        percent = (df.isnull().sum())/(df.isnull().count()*100)\n",
    "        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "        data_type = []\n",
    "        # written by MJ Bahmani\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            data_type.append(dtype)\n",
    "        output['Types'] = data_type\n",
    "        return(np.transpose(output))\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_missing_data(gendered_pronoun_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_pronoun_df[\"num_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_words in data_df 204\n"
     ]
    }
   ],
   "source": [
    "print('maximum of num_words in data_df',gendered_pronoun_df[\"num_words\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min of num_words in data_df 16\n"
     ]
    }
   ],
   "source": [
    "print('min of num_words in data_df',gendered_pronoun_df[\"num_words\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_pronoun_df[\"num_unique_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_unique_words in train 136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     74\n",
       "1     65\n",
       "2     95\n",
       "3     69\n",
       "4    112\n",
       "Name: num_words, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('maximum of num_unique_words in train',gendered_pronoun_df[\"num_unique_words\"].max())\n",
    "\n",
    "gendered_pronoun_df[\"num_words\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61\n",
       "1    58\n",
       "2    71\n",
       "3    58\n",
       "4    80\n",
       "Name: num_unique_words, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df[\"num_unique_words\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_unique_words in train 136\n"
     ]
    }
   ],
   "source": [
    "print('maximum of num_unique_words in train',gendered_pronoun_df[\"num_unique_words\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of num_unique_words in data_df 56.4005\n"
     ]
    }
   ],
   "source": [
    "print('mean of num_unique_words in data_df',gendered_pronoun_df[\"num_unique_words\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_pronoun_df[\"num_chars\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    426\n",
       "1    410\n",
       "2    536\n",
       "3    401\n",
       "4    660\n",
       "Name: num_chars, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df[\"num_chars\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_chars in data_df 1270\n"
     ]
    }
   ],
   "source": [
    "print('maximum of num_chars in data_df',gendered_pronoun_df[\"num_chars\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my own practice trying to find unique characters in data but doesnt make sense is supposed to be max 24  \n",
    "gendered_pronoun_df[\"num_charis\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(set(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df[\"num_charis\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# stopwords cant load has a problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_punctuations in data_df 93\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "gendered_pronoun_df[\"num_punctuations\"] =gendered_pronoun_df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "print('maximum of num_punctuations in data_df',gendered_pronoun_df[\"num_punctuations\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum of num_words_upper in data_df 11\n"
     ]
    }
   ],
   "source": [
    "gendered_pronoun_df[\"num_words_upper\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "print('maximum of num_words_upper in data_df',gendered_pronoun_df[\"num_words_upper\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Text', 'Pronoun', 'Pronoun-offset', 'A', 'A-offset', 'B',\n",
      "       'B-offset', 'URL', 'num_words', 'num_unique_words', 'num_chars',\n",
      "       'num_charis', 'num_punctuations', 'num_words_upper'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(gendered_pronoun_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_charis</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>426</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>410</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "\n",
       "   Pronoun-offset               A  A-offset              B  B-offset  \\\n",
       "0             274  Cheryl Cassidy       191        Pauline       207   \n",
       "1             284       MacKenzie       228  Bernard Leach       251   \n",
       "\n",
       "                                                 URL  num_words  \\\n",
       "0  http://en.wikipedia.org/wiki/List_of_Teachers_...         74   \n",
       "1      http://en.wikipedia.org/wiki/Warren_MacKenzie         65   \n",
       "\n",
       "   num_unique_words  num_chars  num_charis  num_punctuations  num_words_upper  \n",
       "0                61        426          36                14                0  \n",
       "1                58        410          49                12                0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun=gendered_pronoun_df[\"Pronoun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['He', 'Her', 'His', 'She', 'he', 'her', 'him', 'his', 'she'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>URL</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_charis</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>Pronoun_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>426</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>410</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "      <td>95</td>\n",
       "      <td>71</td>\n",
       "      <td>536</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "      <td>69</td>\n",
       "      <td>58</td>\n",
       "      <td>401</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "      <td>112</td>\n",
       "      <td>80</td>\n",
       "      <td>660</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development-6</td>\n",
       "      <td>Sandra Collins is an American DJ. She got her ...</td>\n",
       "      <td>She</td>\n",
       "      <td>411</td>\n",
       "      <td>Collins</td>\n",
       "      <td>236</td>\n",
       "      <td>DJ</td>\n",
       "      <td>347</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Sandra_Collins</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>488</td>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>development-7</td>\n",
       "      <td>Reb Chaim Yaakov's wife is the sister of Rabbi...</td>\n",
       "      <td>his</td>\n",
       "      <td>273</td>\n",
       "      <td>Reb Asher</td>\n",
       "      <td>152</td>\n",
       "      <td>Akiva Eiger</td>\n",
       "      <td>253</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Asher_Arieli</td>\n",
       "      <td>72</td>\n",
       "      <td>48</td>\n",
       "      <td>432</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>development-8</td>\n",
       "      <td>Slant Magazine's Sal Cinquemani viewed the alb...</td>\n",
       "      <td>his</td>\n",
       "      <td>337</td>\n",
       "      <td>Greg Kot</td>\n",
       "      <td>173</td>\n",
       "      <td>Robert Christgau</td>\n",
       "      <td>377</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_Truth_About_L...</td>\n",
       "      <td>71</td>\n",
       "      <td>60</td>\n",
       "      <td>451</td>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>development-9</td>\n",
       "      <td>Her father was an Englishman ``of rank and cul...</td>\n",
       "      <td>her</td>\n",
       "      <td>246</td>\n",
       "      <td>Mary Paine</td>\n",
       "      <td>255</td>\n",
       "      <td>Kelsey</td>\n",
       "      <td>267</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mary_S._Peake</td>\n",
       "      <td>53</td>\n",
       "      <td>42</td>\n",
       "      <td>302</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>development-10</td>\n",
       "      <td>Shaftesbury's UK partners in the production of...</td>\n",
       "      <td>she</td>\n",
       "      <td>329</td>\n",
       "      <td>Christina Jennings</td>\n",
       "      <td>196</td>\n",
       "      <td>Kirstine Stewart</td>\n",
       "      <td>226</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Murdoch_Mysteries</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>381</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>development-11</td>\n",
       "      <td>William Shatner portraying writer Mark Twain; ...</td>\n",
       "      <td>his</td>\n",
       "      <td>300</td>\n",
       "      <td>Peter Mansbridge</td>\n",
       "      <td>168</td>\n",
       "      <td>David Onley</td>\n",
       "      <td>212</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Murdoch_Mysteries</td>\n",
       "      <td>66</td>\n",
       "      <td>58</td>\n",
       "      <td>448</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>development-12</td>\n",
       "      <td>His maternal great-grandfather was Henry Percy...</td>\n",
       "      <td>She</td>\n",
       "      <td>304</td>\n",
       "      <td>Eleanor</td>\n",
       "      <td>217</td>\n",
       "      <td>Eleanor Beauchamp</td>\n",
       "      <td>285</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Henry_Clifford,_2...</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>397</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>development-13</td>\n",
       "      <td>Killian in 1978--79, an assistant district att...</td>\n",
       "      <td>her</td>\n",
       "      <td>293</td>\n",
       "      <td>Williams</td>\n",
       "      <td>247</td>\n",
       "      <td>Mary Helen Moses</td>\n",
       "      <td>273</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Amanda_Williams</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>329</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>development-14</td>\n",
       "      <td>ARTA driver Vitantonio Liuzzi will be replaced...</td>\n",
       "      <td>his</td>\n",
       "      <td>340</td>\n",
       "      <td>Kazuki Nakajima</td>\n",
       "      <td>275</td>\n",
       "      <td>Oliver Jarvis</td>\n",
       "      <td>297</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2015_Super_GT_Series</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>396</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>development-15</td>\n",
       "      <td>Twenty years ago, Lorenzo Uribe discovered tru...</td>\n",
       "      <td>her</td>\n",
       "      <td>250</td>\n",
       "      <td>Maria</td>\n",
       "      <td>219</td>\n",
       "      <td>Gracia</td>\n",
       "      <td>235</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Malparida</td>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>350</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>development-16</td>\n",
       "      <td>A colleague in the department run by Erwin Fri...</td>\n",
       "      <td>she</td>\n",
       "      <td>295</td>\n",
       "      <td>Alma Taggart</td>\n",
       "      <td>192</td>\n",
       "      <td>McCulloch</td>\n",
       "      <td>219</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Lucia_McCulloch</td>\n",
       "      <td>61</td>\n",
       "      <td>52</td>\n",
       "      <td>379</td>\n",
       "      <td>45</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>development-17</td>\n",
       "      <td>Maurice custom-tailors clothing for de Var*ze ...</td>\n",
       "      <td>him</td>\n",
       "      <td>384</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>334</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>365</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Love_Me_Tonight</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>454</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>development-18</td>\n",
       "      <td>In 1988, Race suffered an abdominal injury and...</td>\n",
       "      <td>his</td>\n",
       "      <td>330</td>\n",
       "      <td>Randy Savage</td>\n",
       "      <td>181</td>\n",
       "      <td>Ted DiBiase</td>\n",
       "      <td>235</td>\n",
       "      <td>http://en.wikipedia.org/wiki/King_of_the_Ring</td>\n",
       "      <td>74</td>\n",
       "      <td>58</td>\n",
       "      <td>436</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>development-19</td>\n",
       "      <td>The rest of the group find out what has happen...</td>\n",
       "      <td>him</td>\n",
       "      <td>335</td>\n",
       "      <td>Justin</td>\n",
       "      <td>281</td>\n",
       "      <td>Tim</td>\n",
       "      <td>296</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Never_Back_Down_2...</td>\n",
       "      <td>67</td>\n",
       "      <td>54</td>\n",
       "      <td>359</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>development-20</td>\n",
       "      <td>On 30 July 1966, Ramsey's promise was fulfille...</td>\n",
       "      <td>his</td>\n",
       "      <td>476</td>\n",
       "      <td>Greaves</td>\n",
       "      <td>330</td>\n",
       "      <td>Ramsey</td>\n",
       "      <td>381</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Alf_Ramsey</td>\n",
       "      <td>100</td>\n",
       "      <td>78</td>\n",
       "      <td>586</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>development-21</td>\n",
       "      <td>Ben Severson is a pioneer in the sport of body...</td>\n",
       "      <td>He</td>\n",
       "      <td>234</td>\n",
       "      <td>Mike Stewart</td>\n",
       "      <td>113</td>\n",
       "      <td>Ben</td>\n",
       "      <td>150</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Ben_Severson</td>\n",
       "      <td>57</td>\n",
       "      <td>47</td>\n",
       "      <td>320</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>development-22</td>\n",
       "      <td>This device allowed him to continue his racing...</td>\n",
       "      <td>his</td>\n",
       "      <td>304</td>\n",
       "      <td>Kurt</td>\n",
       "      <td>317</td>\n",
       "      <td>Paul Newman</td>\n",
       "      <td>338</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kurt_Kossmann</td>\n",
       "      <td>70</td>\n",
       "      <td>56</td>\n",
       "      <td>426</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>development-23</td>\n",
       "      <td>The meeting went a little differently from wha...</td>\n",
       "      <td>his</td>\n",
       "      <td>430</td>\n",
       "      <td>Magee</td>\n",
       "      <td>296</td>\n",
       "      <td>Kenneth Grant</td>\n",
       "      <td>386</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mike_Magee_(journ...</td>\n",
       "      <td>91</td>\n",
       "      <td>68</td>\n",
       "      <td>540</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>development-24</td>\n",
       "      <td>Louis McNeill begin her publishing career sell...</td>\n",
       "      <td>her</td>\n",
       "      <td>447</td>\n",
       "      <td>Maggie Anderson</td>\n",
       "      <td>273</td>\n",
       "      <td>McNeill</td>\n",
       "      <td>301</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Louise_McNeill</td>\n",
       "      <td>75</td>\n",
       "      <td>62</td>\n",
       "      <td>481</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>development-25</td>\n",
       "      <td>Her initial ambition was to become a ``culture...</td>\n",
       "      <td>her</td>\n",
       "      <td>329</td>\n",
       "      <td>Hartwig</td>\n",
       "      <td>210</td>\n",
       "      <td>Margaret Bell</td>\n",
       "      <td>266</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Marie_Hartwig</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>333</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>development-26</td>\n",
       "      <td>They were broadly positive, though they stoppe...</td>\n",
       "      <td>his</td>\n",
       "      <td>299</td>\n",
       "      <td>Michael Billington</td>\n",
       "      <td>92</td>\n",
       "      <td>Martin</td>\n",
       "      <td>266</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Life_and_Beth</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>363</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>development-27</td>\n",
       "      <td>Connie, in a one-sided conversation, gives pra...</td>\n",
       "      <td>her</td>\n",
       "      <td>280</td>\n",
       "      <td>Beth</td>\n",
       "      <td>238</td>\n",
       "      <td>Connie</td>\n",
       "      <td>263</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Life_and_Beth</td>\n",
       "      <td>81</td>\n",
       "      <td>61</td>\n",
       "      <td>490</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>development-28</td>\n",
       "      <td>This is a list of episodes from the MTV show B...</td>\n",
       "      <td>his</td>\n",
       "      <td>194</td>\n",
       "      <td>Alan</td>\n",
       "      <td>204</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>223</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Bully_Bea...</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>290</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>development-29</td>\n",
       "      <td>Meet Mike, the shortest bully to appear on the...</td>\n",
       "      <td>him</td>\n",
       "      <td>250</td>\n",
       "      <td>Mayhem Miller</td>\n",
       "      <td>153</td>\n",
       "      <td>Eddie Alvarez</td>\n",
       "      <td>189</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Bully_Bea...</td>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>263</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>development-30</td>\n",
       "      <td>With only six months' experience under his bel...</td>\n",
       "      <td>his</td>\n",
       "      <td>294</td>\n",
       "      <td>Paul</td>\n",
       "      <td>241</td>\n",
       "      <td>Freddie Griffiths</td>\n",
       "      <td>258</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Tommy_Paul</td>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>323</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               Text Pronoun  \\\n",
       "0    development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1    development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2    development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3    development-4  The current members of Crime have also perform...     his   \n",
       "4    development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "5    development-6  Sandra Collins is an American DJ. She got her ...     She   \n",
       "6    development-7  Reb Chaim Yaakov's wife is the sister of Rabbi...     his   \n",
       "7    development-8  Slant Magazine's Sal Cinquemani viewed the alb...     his   \n",
       "8    development-9  Her father was an Englishman ``of rank and cul...     her   \n",
       "9   development-10  Shaftesbury's UK partners in the production of...     she   \n",
       "10  development-11  William Shatner portraying writer Mark Twain; ...     his   \n",
       "11  development-12  His maternal great-grandfather was Henry Percy...     She   \n",
       "12  development-13  Killian in 1978--79, an assistant district att...     her   \n",
       "13  development-14  ARTA driver Vitantonio Liuzzi will be replaced...     his   \n",
       "14  development-15  Twenty years ago, Lorenzo Uribe discovered tru...     her   \n",
       "15  development-16  A colleague in the department run by Erwin Fri...     she   \n",
       "16  development-17  Maurice custom-tailors clothing for de Var*ze ...     him   \n",
       "17  development-18  In 1988, Race suffered an abdominal injury and...     his   \n",
       "18  development-19  The rest of the group find out what has happen...     him   \n",
       "19  development-20  On 30 July 1966, Ramsey's promise was fulfille...     his   \n",
       "20  development-21  Ben Severson is a pioneer in the sport of body...      He   \n",
       "21  development-22  This device allowed him to continue his racing...     his   \n",
       "22  development-23  The meeting went a little differently from wha...     his   \n",
       "23  development-24  Louis McNeill begin her publishing career sell...     her   \n",
       "24  development-25  Her initial ambition was to become a ``culture...     her   \n",
       "25  development-26  They were broadly positive, though they stoppe...     his   \n",
       "26  development-27  Connie, in a one-sided conversation, gives pra...     her   \n",
       "27  development-28  This is a list of episodes from the MTV show B...     his   \n",
       "28  development-29  Meet Mike, the shortest bully to appear on the...     him   \n",
       "29  development-30  With only six months' experience under his bel...     his   \n",
       "\n",
       "    Pronoun-offset                   A  A-offset                  B  B-offset  \\\n",
       "0              274      Cheryl Cassidy       191            Pauline       207   \n",
       "1              284           MacKenzie       228      Bernard Leach       251   \n",
       "2              265             Angeloz       173         De la Sota       246   \n",
       "3              321                Hell       174    Henry Rosenthal       336   \n",
       "4              437   Kitty Oppenheimer       219             Rivera       294   \n",
       "5              411             Collins       236                 DJ       347   \n",
       "6              273           Reb Asher       152        Akiva Eiger       253   \n",
       "7              337            Greg Kot       173   Robert Christgau       377   \n",
       "8              246          Mary Paine       255             Kelsey       267   \n",
       "9              329  Christina Jennings       196   Kirstine Stewart       226   \n",
       "10             300    Peter Mansbridge       168        David Onley       212   \n",
       "11             304             Eleanor       217  Eleanor Beauchamp       285   \n",
       "12             293            Williams       247   Mary Helen Moses       273   \n",
       "13             340     Kazuki Nakajima       275      Oliver Jarvis       297   \n",
       "14             250               Maria       219             Gracia       235   \n",
       "15             295        Alma Taggart       192          McCulloch       219   \n",
       "16             384             Maurice       334            Gilbert       365   \n",
       "17             330        Randy Savage       181        Ted DiBiase       235   \n",
       "18             335              Justin       281                Tim       296   \n",
       "19             476             Greaves       330             Ramsey       381   \n",
       "20             234        Mike Stewart       113                Ben       150   \n",
       "21             304                Kurt       317        Paul Newman       338   \n",
       "22             430               Magee       296      Kenneth Grant       386   \n",
       "23             447     Maggie Anderson       273            McNeill       301   \n",
       "24             329             Hartwig       210      Margaret Bell       266   \n",
       "25             299  Michael Billington        92             Martin       266   \n",
       "26             280                Beth       238             Connie       263   \n",
       "27             194                Alan       204               Ryan       223   \n",
       "28             250       Mayhem Miller       153      Eddie Alvarez       189   \n",
       "29             294                Paul       241  Freddie Griffiths       258   \n",
       "\n",
       "                                                  URL  num_words  \\\n",
       "0   http://en.wikipedia.org/wiki/List_of_Teachers_...         74   \n",
       "1       http://en.wikipedia.org/wiki/Warren_MacKenzie         65   \n",
       "2   http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...         95   \n",
       "3           http://en.wikipedia.org/wiki/Crime_(band)         69   \n",
       "4         http://en.wikipedia.org/wiki/Jessica_Rivera        112   \n",
       "5         http://en.wikipedia.org/wiki/Sandra_Collins         81   \n",
       "6           http://en.wikipedia.org/wiki/Asher_Arieli         72   \n",
       "7   http://en.wikipedia.org/wiki/The_Truth_About_L...         71   \n",
       "8          http://en.wikipedia.org/wiki/Mary_S._Peake         53   \n",
       "9      http://en.wikipedia.org/wiki/Murdoch_Mysteries         52   \n",
       "10     http://en.wikipedia.org/wiki/Murdoch_Mysteries         66   \n",
       "11  http://en.wikipedia.org/wiki/Henry_Clifford,_2...         60   \n",
       "12       http://en.wikipedia.org/wiki/Amanda_Williams         52   \n",
       "13  http://en.wikipedia.org/wiki/2015_Super_GT_Series         63   \n",
       "14             http://en.wikipedia.org/wiki/Malparida         59   \n",
       "15       http://en.wikipedia.org/wiki/Lucia_McCulloch         61   \n",
       "16       http://en.wikipedia.org/wiki/Love_Me_Tonight         74   \n",
       "17      http://en.wikipedia.org/wiki/King_of_the_Ring         74   \n",
       "18  http://en.wikipedia.org/wiki/Never_Back_Down_2...         67   \n",
       "19            http://en.wikipedia.org/wiki/Alf_Ramsey        100   \n",
       "20          http://en.wikipedia.org/wiki/Ben_Severson         57   \n",
       "21         http://en.wikipedia.org/wiki/Kurt_Kossmann         70   \n",
       "22  http://en.wikipedia.org/wiki/Mike_Magee_(journ...         91   \n",
       "23        http://en.wikipedia.org/wiki/Louise_McNeill         75   \n",
       "24         http://en.wikipedia.org/wiki/Marie_Hartwig         51   \n",
       "25         http://en.wikipedia.org/wiki/Life_and_Beth         55   \n",
       "26         http://en.wikipedia.org/wiki/Life_and_Beth         81   \n",
       "27  http://en.wikipedia.org/wiki/List_of_Bully_Bea...         54   \n",
       "28  http://en.wikipedia.org/wiki/List_of_Bully_Bea...         52   \n",
       "29            http://en.wikipedia.org/wiki/Tommy_Paul         52   \n",
       "\n",
       "    num_unique_words  num_chars  num_charis  num_punctuations  \\\n",
       "0                 61        426          36                14   \n",
       "1                 58        410          49                12   \n",
       "2                 71        536          47                16   \n",
       "3                 58        401          42                13   \n",
       "4                 80        660          53                18   \n",
       "5                 64        488          43                13   \n",
       "6                 48        432          37                11   \n",
       "7                 60        451          46                21   \n",
       "8                 42        302          41                13   \n",
       "9                 45        381          39                14   \n",
       "10                58        448          42                 9   \n",
       "11                40        397          41                10   \n",
       "12                42        329          44                13   \n",
       "13                53        396          46                 7   \n",
       "14                47        350          34                12   \n",
       "15                52        379          45                14   \n",
       "16                60        454          41                17   \n",
       "17                58        436          40                10   \n",
       "18                54        359          34                 6   \n",
       "19                78        586          49                18   \n",
       "20                47        320          34                 5   \n",
       "21                56        426          46                 7   \n",
       "22                68        540          51                 7   \n",
       "23                62        481          47                14   \n",
       "24                47        333          35                15   \n",
       "25                50        363          33                14   \n",
       "26                61        490          34                15   \n",
       "27                47        290          37                 6   \n",
       "28                46        263          38                 6   \n",
       "29                44        323          44                 7   \n",
       "\n",
       "    num_words_upper  Pronoun_binary  \n",
       "0                 0             4.0  \n",
       "1                 0             2.0  \n",
       "2                 0             2.0  \n",
       "3                 1             2.0  \n",
       "4                 1             1.0  \n",
       "5                 4             1.0  \n",
       "6                 0             2.0  \n",
       "7                 0             2.0  \n",
       "8                 0             4.0  \n",
       "9                 4             1.0  \n",
       "10                0             2.0  \n",
       "11                0             1.0  \n",
       "12                0             4.0  \n",
       "13                2             2.0  \n",
       "14                0             4.0  \n",
       "15                3             1.0  \n",
       "16                0             3.0  \n",
       "17                0             2.0  \n",
       "18                0             3.0  \n",
       "19                1             2.0  \n",
       "20                0             0.0  \n",
       "21                0             2.0  \n",
       "22                1             2.0  \n",
       "23                0             4.0  \n",
       "24                1             4.0  \n",
       "25                0             2.0  \n",
       "26                0             4.0  \n",
       "27                1             2.0  \n",
       "28                1             3.0  \n",
       "29                1             2.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## is suggested by  https://www.kaggle.com/aavella77\n",
    "binary = {\n",
    "    \"He\": 0,\n",
    "    \"he\": 0,\n",
    "    \"She\": 1,\n",
    "    \"she\": 1,\n",
    "    \"His\": 2,\n",
    "    \"his\": 2,\n",
    "    \"Him\": 3,\n",
    "    \"him\": 3,\n",
    "    \"Her\": 4,\n",
    "    \"her\": 4\n",
    "}\n",
    "for index in range(len(gendered_pronoun_df)):\n",
    "    key = gendered_pronoun_df.iloc[index]['Pronoun']\n",
    "    gendered_pronoun_df.at[index, 'Pronoun_binary'] = binary[key]\n",
    "gendered_pronoun_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gendered_pronoun_df.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_text=gendered_pronoun_df.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kadas/nltk_data'\n    - '/home/kadas/anaconda3/nltk_data'\n    - '/home/kadas/anaconda3/share/nltk_data'\n    - '/home/kadas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e1f3e082aec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kadas/nltk_data'\n    - '/home/kadas/anaconda3/nltk_data'\n    - '/home/kadas/anaconda3/share/nltk_data'\n    - '/home/kadas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(our_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kadas/nltk_data'\n    - '/home/kadas/anaconda3/nltk_data'\n    - '/home/kadas/anaconda3/share/nltk_data'\n    - '/home/kadas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2a6ae2ff4bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/kadas/nltk_data'\n    - '/home/kadas/anaconda3/nltk_data'\n    - '/home/kadas/anaconda3/share/nltk_data'\n    - '/home/kadas/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(our_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
